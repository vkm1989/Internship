{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a006acbd",
   "metadata": {},
   "source": [
    "# WEB SCRAPING – ASSIGNMENT 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ba74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: outcome in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.0.0rc9)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arso (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb25b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee48b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the driver\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\hp\\Desktop\\\\task\\\\driver\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a871e",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb02ca7",
   "metadata": {},
   "source": [
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ \n",
    "2.\tEnter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field. \n",
    "3.\tThen click the search button. \n",
    "4.\tThen scrape the data for the first 10 jobs results you get. \n",
    "5.\tFinally create a dataframe of the scraped data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8696d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri.com page and automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e74a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation \n",
    "\n",
    "desig=driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "desig.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3be836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter location\n",
    "\n",
    "loc=driver.find_element(By.XPATH, \"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "loc.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a227ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering search button\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d180e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list, scrap the data for the job reasults.\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experiance_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b40cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title \n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "    \n",
    "# scraping job location\n",
    "\n",
    "loc_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in loc_tags[0:10]:\n",
    "    loc=i.text\n",
    "    job_location.append(loc)\n",
    "    \n",
    "    \n",
    "# scraping company name\n",
    "\n",
    "comp_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in comp_tags[0:10]:\n",
    "    comp=i.text\n",
    "    company_name.append(comp)\n",
    "    \n",
    "    \n",
    "    \n",
    "# scraping experiance\n",
    "\n",
    "exp_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exp_tags[0:10]:\n",
    "    exp=i.text\n",
    "    experiance_required.append(exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f47770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# print the length of scrap data\n",
    "\n",
    "print(len(job_title), len(job_location), len(company_name), len(experiance_required))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623728ae",
   "metadata": {},
   "source": [
    "MAKE A DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc9987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQUIRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai</td>\n",
       "      <td>Latentview</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - CRM Platform</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>Artech infosystem</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For Data Analyst (DA)/ Team Lead (TL) -...</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>Cognizant</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Payroll Transformation Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Arrow Electronics</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Master Data Management Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analytics and Interpretation Business Ana...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Goalreify Ventures</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Goalreify Ventures</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0                                Senior Data Analyst   \n",
       "1                        Data Analyst - CRM Platform   \n",
       "2  Hiring For Data Analyst (DA)/ Team Lead (TL) -...   \n",
       "3                Payroll Transformation Data Analyst   \n",
       "4            Master Data Management Business Analyst   \n",
       "5  Data Analytics and Interpretation Business Ana...   \n",
       "6                                       Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                                Senior Data Analyst   \n",
       "9                                Senior Data Analyst   \n",
       "\n",
       "                                        JOB_LOCATION        COMPANY_NAME  \\\n",
       "0                       Bangalore/Bengaluru, Chennai          Latentview   \n",
       "1  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   Artech infosystem   \n",
       "2  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...           Cognizant   \n",
       "3                                Bangalore/Bengaluru   Arrow Electronics   \n",
       "4                                Bangalore/Bengaluru           Accenture   \n",
       "5                                Bangalore/Bengaluru           Accenture   \n",
       "6                                Bangalore/Bengaluru                 ANZ   \n",
       "7                                Bangalore/Bengaluru                 ANZ   \n",
       "8                                Bangalore/Bengaluru  Goalreify Ventures   \n",
       "9                                Bangalore/Bengaluru  Goalreify Ventures   \n",
       "\n",
       "  EXP_REQUIRED  \n",
       "0      3-6 Yrs  \n",
       "1      1-6 Yrs  \n",
       "2      3-8 Yrs  \n",
       "3     5-10 Yrs  \n",
       "4      6-8 Yrs  \n",
       "5      6-8 Yrs  \n",
       "6     7-12 Yrs  \n",
       "7     7-12 Yrs  \n",
       "8      3-6 Yrs  \n",
       "9      4-6 Yrs  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data frame\n",
    "\n",
    "df = pd.DataFrame({'JOB_TITLE':job_title, 'JOB_LOCATION':job_location, 'COMPANY_NAME':company_name, 'EXP_REQUIRED':experiance_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce17ab8",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914e167",
   "metadata": {},
   "source": [
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ \n",
    "2.\tEnter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field. \n",
    "3.\tThen click the search button. \n",
    "4.\tThen scrape the data for the first 10 jobs results you get. \n",
    "5.\tFinally create a dataframe of the scraped data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3a6449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation \n",
    "\n",
    "desig_new = driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "desig_new.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfba3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter location\n",
    "\n",
    "loc_new=driver.find_element(By.XPATH, \"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "loc_new.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "448c92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering search button\n",
    "\n",
    "search1=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba1f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list, scrap the data for the job reasults.\n",
    "\n",
    "job_title_1=[]\n",
    "job_location_1=[]\n",
    "company_name_1=[]\n",
    "experiance_required_1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e5a5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title \n",
    "\n",
    "T_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in T_tags[0:10]:\n",
    "    title_1=i.text\n",
    "    job_title_1.append(title_1)\n",
    "    \n",
    "    \n",
    "# scraping job location\n",
    "\n",
    "Lc_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in Lc_tags[0:10]:\n",
    "    loc_1=i.text\n",
    "    job_location_1.append(loc_1)\n",
    "    \n",
    "    \n",
    "# scraping company name\n",
    "\n",
    "Cmp_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in Cmp_tags[0:10]:\n",
    "    comp_1=i.text\n",
    "    company_name_1.append(comp_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "# scraping experiance\n",
    "\n",
    "expri_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in expri_tags[0:10]:\n",
    "    exp_1=i.text\n",
    "    experiance_required_1.append(exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98127a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title_1), len(job_location_1), len(company_name_1), len(experiance_required_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd71e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQUIRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mongodb Database Administrator, Maria DB or Ca...</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad, P...</td>\n",
       "      <td>Mphasis</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad, P...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>10-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, New Delhi, Chennai</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Industry X - Software Engineering</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Module Lead - BIDW</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Mphasis</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist: Artificial Intelligence</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0                            Data Science Specialist   \n",
       "1                               Data Science Manager   \n",
       "2  Mongodb Database Administrator, Maria DB or Ca...   \n",
       "3                   Analystics & Modeling Specialist   \n",
       "4                   Assistant Manager - Data Science   \n",
       "5                                     Data Scientist   \n",
       "6                              Senior Data Scientist   \n",
       "7                  Industry X - Software Engineering   \n",
       "8                                 Module Lead - BIDW   \n",
       "9            Data Scientist: Artificial Intelligence   \n",
       "\n",
       "                                        JOB_LOCATION             COMPANY_NAME  \\\n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture   \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture   \n",
       "2  Bangalore/Bengaluru, Hyderabad/Secunderabad, P...                  Mphasis   \n",
       "3  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture   \n",
       "4                  Bangalore/Bengaluru, Mumbai, Pune               CitiusTech   \n",
       "5  Bangalore/Bengaluru, Hyderabad/Secunderabad, P...            Tech Mahindra   \n",
       "6    Bangalore/Bengaluru, Mumbai, New Delhi, Chennai  Boston Consulting Group   \n",
       "7                                Bangalore/Bengaluru                Accenture   \n",
       "8                                Bangalore/Bengaluru                  Mphasis   \n",
       "9                                Bangalore/Bengaluru                      IBM   \n",
       "\n",
       "  EXP_REQUIRED  \n",
       "0      2-4 Yrs  \n",
       "1      4-7 Yrs  \n",
       "2     9-14 Yrs  \n",
       "3      6-8 Yrs  \n",
       "4      5-9 Yrs  \n",
       "5    10-14 Yrs  \n",
       "6     5-10 Yrs  \n",
       "7      3-5 Yrs  \n",
       "8      5-8 Yrs  \n",
       "9      4-8 Yrs  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data frame\n",
    "\n",
    "df_1 = pd.DataFrame({'JOB_TITLE':job_title_1, 'JOB_LOCATION':job_location_1, 'COMPANY_NAME':company_name_1, 'EXP_REQUIRED':experiance_required_1})\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a5ec4",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c1d86",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter. \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs \n",
    "The task will be done as shown in the below steps: \n",
    "1.\tfirst get the webpage https://www.naukri.com/ \n",
    "2.\tEnter “Data Scientist” in “Skill, Designations, and Companies” field. \n",
    "3.\tThen click the search button. \n",
    "4.\tThen apply the location filter and salary filter by checking the respective boxes 5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37358281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation \n",
    "\n",
    "desig_new_1 = driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "desig_new_1.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd3aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter location\n",
    "\n",
    "#loc_new_1=driver.find_element(By.XPATH, \"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "#loc_new_1.send_keys('Delhi/NCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aca7fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering search button\n",
    "\n",
    "search_1=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14acc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list, scrap the data for the job reasults.\n",
    "\n",
    "job_title_2=[]\n",
    "job_location_2=[]\n",
    "company_name_2=[]\n",
    "experiance_required_2=[]\n",
    "salary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b436f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title \n",
    "\n",
    "jobs_tit = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in jobs_tit[0:10]:\n",
    "    tit = i.text\n",
    "    job_title_2.append(tit)\n",
    "    \n",
    "    \n",
    "# scraping job location\n",
    "\n",
    "Loca  =driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in Loca[0:10]:\n",
    "    locs=i.text\n",
    "    job_location_2.append(locs)\n",
    "    \n",
    "    \n",
    "# scraping company name\n",
    "\n",
    "Cmp_name = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in Cmp_name[0:10]:\n",
    "    name=i.text\n",
    "    company_name_2.append(name)\n",
    "    \n",
    "    \n",
    "    \n",
    "# scraping experiance\n",
    "\n",
    "exp = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exp[0:10]:\n",
    "    ex=i.text\n",
    "    experiance_required_2.append(ex)\n",
    "    \n",
    "    \n",
    "# scraping salary\n",
    "\n",
    "sal = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi salary\"]//span')\n",
    "for i in sal[0:10]:\n",
    "    sa=i.text\n",
    "    salary.append(sa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a02be052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title_2), len(job_location_2), len(company_name_2), len(experiance_required_2), len(salary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ccdb6",
   "metadata": {},
   "source": [
    "MAKE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4214b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>JOB_LOCATION</th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXP_REQUIRED</th>\n",
       "      <th>SALARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Nagpur, Bangalore/Bengaluru</td>\n",
       "      <td>GlobalLogic</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "      <td>3,00,000 - 4,50,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Noida(Sector-59 Noida)\\n(WFH during Covid)</td>\n",
       "      <td>R Systems International</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>IHS Markit</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>5,00,000 - 12,00,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Activation Specialist - Adobe Target</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>NGI Ventures</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...</td>\n",
       "      <td>torcai digital media</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   JOB_TITLE  \\\n",
       "0                             Data Scientist   \n",
       "1            DigitalBCG GAMMA Data Scientist   \n",
       "2                        Lead Data Scientist   \n",
       "3                             Data Scientist   \n",
       "4          Data Scientist - Engine Algorithm   \n",
       "5                             Data Scientist   \n",
       "6        Data Scientist / Chat-bot Developer   \n",
       "7  Data Activation Specialist - Adobe Target   \n",
       "8                             Data Scientist   \n",
       "9                             Data Scientist   \n",
       "\n",
       "                                        JOB_LOCATION             COMPANY_NAME  \\\n",
       "0                 Noida, Nagpur, Bangalore/Bengaluru              GlobalLogic   \n",
       "1                     New Delhi, Bangalore/Bengaluru  Boston Consulting Group   \n",
       "2         Noida(Sector-59 Noida)\\n(WFH during Covid)  R Systems International   \n",
       "3                                   Gurgaon/Gurugram               IHS Markit   \n",
       "4  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...             Primo Hiring   \n",
       "5                                   Gurgaon/Gurugram                    Optum   \n",
       "6  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...             Big Seo Buzz   \n",
       "7  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...           Okda Solutions   \n",
       "8                                              Noida             NGI Ventures   \n",
       "9  Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...     torcai digital media   \n",
       "\n",
       "  EXP_REQUIRED                    SALARY  \n",
       "0     8-10 Yrs   3,00,000 - 4,50,000 PA.  \n",
       "1      2-5 Yrs             Not disclosed  \n",
       "2     7-10 Yrs             Not disclosed  \n",
       "3      3-6 Yrs             Not disclosed  \n",
       "4      1-3 Yrs             Not disclosed  \n",
       "5      2-7 Yrs             Not disclosed  \n",
       "6      3-7 Yrs  5,00,000 - 12,00,000 PA.  \n",
       "7     7-10 Yrs             Not disclosed  \n",
       "8      0-5 Yrs             Not disclosed  \n",
       "9      2-7 Yrs             Not disclosed  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "\n",
    "df_2 = pd.DataFrame({'JOB_TITLE':job_title_2, 'JOB_LOCATION':job_location_2, 'COMPANY_NAME':company_name_2, 'EXP_REQUIRED':experiance_required_2, 'SALARY':salary})\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23af911",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a802d62",
   "metadata": {},
   "source": [
    "1.\tBrand \n",
    "2.\tProduct Description \n",
    "3.\tPrice \n",
    "The attributes which you have to scrape is ticked marked in the below image\n",
    "\n",
    "To scrape the data you have to go through following steps: \n",
    "1.\tGo to Flipkart webpage by url : https://www.flipkart.com/ \n",
    "2.\tEnter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon \n",
    "3.\tAfter that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual. \n",
    "4.\tAfter scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it. \n",
    "5.\tNow scrape data from this page as usual \n",
    "6.\tRepeat this until you get data for 100 sunglasses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "795643bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart.com page and automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81f4fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product=driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "product.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e249a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME, \"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b247a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping next page title\n",
    "\n",
    "brand_name = [] # empty list\n",
    "features = []\n",
    "price = []\n",
    "discount =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ee9906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=3\n",
    "\n",
    "for page in range(start, end):\n",
    "    \n",
    "    \n",
    "    brand = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand:\n",
    "        #bn=i.text\n",
    "        brand_name.append(i.text)\n",
    "#next_button = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "#next_button.click()    \n",
    "        \n",
    "    featu = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in featu:\n",
    "        #fn=i.text\n",
    "        features.append(i.text)\n",
    "        \n",
    "        \n",
    "    pric = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in pric:\n",
    "        #pr=i.text\n",
    "        price.append(i.text)\n",
    "        \n",
    "        \n",
    "    disc = driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in disc:\n",
    "        #dp=i.text\n",
    "        discount.append(i.text)\n",
    "        \n",
    "# scrap data from next page    \n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5bdd7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "print(len(brand_name), len(features), len(price), len(discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc4a502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUNGLASS</th>\n",
       "      <th>FEATURES</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹899</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Shield Sung...</td>\n",
       "      <td>₹255</td>\n",
       "      <td>91% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Butterfly Sunglasses (60)</td>\n",
       "      <td>₹280</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Rectangular, Over-sized Sunglass...</td>\n",
       "      <td>₹250</td>\n",
       "      <td>91% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection Round, Shield Sunglasses (55)</td>\n",
       "      <td>₹79</td>\n",
       "      <td>92% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>FOSSIL</td>\n",
       "      <td>Others Aviator Sunglasses (58)</td>\n",
       "      <td>₹2,899</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹899</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Shield Sung...</td>\n",
       "      <td>₹255</td>\n",
       "      <td>91% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Butterfly Sunglasses (60)</td>\n",
       "      <td>₹280</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹801</td>\n",
       "      <td>59% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SUNGLASS                                           FEATURES  \\\n",
       "0     VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "1    ROZZETTA CRAFT  UV Protection, Gradient Butterfly, Shield Sung...   \n",
       "2            PIRASO            UV Protection Butterfly Sunglasses (60)   \n",
       "3            PIRASO  UV Protection Rectangular, Over-sized Sunglass...   \n",
       "4             NuVew        UV Protection Round, Shield Sunglasses (55)   \n",
       "..              ...                                                ...   \n",
       "115          FOSSIL                     Others Aviator Sunglasses (58)   \n",
       "116   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "117  ROZZETTA CRAFT  UV Protection, Gradient Butterfly, Shield Sung...   \n",
       "118          PIRASO            UV Protection Butterfly Sunglasses (60)   \n",
       "119   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...   \n",
       "\n",
       "      PRICE DISCOUNT  \n",
       "0      ₹899  55% off  \n",
       "1      ₹255  91% off  \n",
       "2      ₹280  89% off  \n",
       "3      ₹250  91% off  \n",
       "4       ₹79  92% off  \n",
       "..      ...      ...  \n",
       "115  ₹2,899  66% off  \n",
       "116    ₹899  55% off  \n",
       "117    ₹255  91% off  \n",
       "118    ₹280  89% off  \n",
       "119    ₹801  59% off  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "\n",
    "df_3 = pd.DataFrame({'SUNGLASS':brand_name, 'FEATURES':features, 'PRICE':price, 'DISCOUNT':discount})\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1c834",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045898f",
   "metadata": {},
   "source": [
    "This task will be done in following steps: \n",
    "1. First get the webpage  https://www.flipkart.com/ \n",
    "2.\tEnter “iphone 11” in “Search” field .  \n",
    "3.\tThen click the search button.\n",
    "\n",
    "1.\tRating \n",
    "2.\tReview summary \n",
    "3.\tFull review \n",
    "4.\tYou have to scrape this data for first 100 reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208ef47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching element\n",
    "\n",
    "product=driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "product.send_keys('iphone 11 mobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a52e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search product by click\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME, \"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22185600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap mobile rating\n",
    "\n",
    "i_ph_rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a15807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping i phone rating from the given page\n",
    "st=0\n",
    "en=4\n",
    "\n",
    "\n",
    "for page in range(st, en):\n",
    "    \n",
    "    rating = driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK\"]')\n",
    "    for i in rating:\n",
    "        i_ph_rating.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_1 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_1.click()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ef417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.4', '4.4', '4.5', '4.5', '4.4', '4.6', '3.9', '4.6', '4.6', '4.6', '4.5', '4.6', '4.6', '4.6', '4.6', '4.4', '4.4', '4.6', '4.5', '4.5', '4.7', '4.7', '4.7', '4.6', '4.4', '4.6', '4.7', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.5', '4.7', '4.7', '4.5', '4.6', '4.6', '4.6', '4.7', '4.4', '4.6', '4.4', '3.9', '3.9', '4.4', '4.4', '4.6', '4.7', '4.6', '4.4', '4.4', '4.5', '4.5', '4.4', '4.6', '3.9', '4.6', '4.6', '4.6', '4.5', '4.6', '4.6', '4.6', '4.6', '4.4', '4.4', '4.6', '4.5', '4.5', '4.7', '4.7', '4.7', '4.6', '4.4', '4.6', '4.7', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.6', '4.5', '4.7', '4.7', '4.5', '4.6', '4.6']\n"
     ]
    }
   ],
   "source": [
    "print(i_ph_rating[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9dda142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap mobile review\n",
    "\n",
    "ph_review_sum=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40eba802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping i phone review summary from the given page\n",
    "sr=0\n",
    "ed=12\n",
    "\n",
    "\n",
    "for page in range(sr, ed):\n",
    "    \n",
    "    review = driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review:\n",
    "        ph_review_sum.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_1 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_1.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e6a3ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perfect product!',\n",
       " 'Terrific',\n",
       " 'Value-for-money',\n",
       " 'Wonderful',\n",
       " 'Good quality product',\n",
       " 'Perfect product!',\n",
       " 'Fabulous!',\n",
       " 'Mind-blowing purchase',\n",
       " 'Perfect product!',\n",
       " 'Terrific',\n",
       " 'Value-for-money',\n",
       " 'Wonderful',\n",
       " 'Good quality product',\n",
       " 'Perfect product!',\n",
       " 'Fabulous!',\n",
       " 'Mind-blowing purchase',\n",
       " 'Perfect product!',\n",
       " 'Terrific',\n",
       " 'Value-for-money',\n",
       " 'Wonderful']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ph_review_sum[0:100]\n",
    "ph_review_sum[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9671929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap mobile full review\n",
    "\n",
    "ph_full_review=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e4751eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping i phone full review  from the given page\n",
    "sa=0\n",
    "ee=12\n",
    "\n",
    "\n",
    "for page in range(sa, ee):\n",
    "    \n",
    "    full_review = driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in full_review:\n",
    "        ph_full_review.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_2 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_2.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4360b8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.',\n",
       " \"After using 3 years mobile review. Excellent & Awesome Mobile fully I love it mobile don't wait take it perfectly suits everyone very stylish and operating a...\\nRead full review\",\n",
       " 'I am using the phone for last 5 years and found no cons as such. The camera quality is bit low as compared to other iphone varient but it can be managable. B...\\nRead full review',\n",
       " \"I'm Really happy with the product\\nDelivery was fast as well\\n..it was a gift for my sister and she loved it so much.\",\n",
       " 'Nice product..just love it',\n",
       " \"impressively Nice......\\nOne of the greatest iPhone i ever used ....\\nAll was like Never before ...\\nit's just Amazing ...\\nBattery Life is too good ...2 Days wi...\\nRead full review\",\n",
       " 'Nice products thanks flkat',\n",
       " 'Fast performance to previous iPhone x\\nGood camera quality but the best part of night mode #killing\\n\\nI am already One-plus7\\nHuge difference between in night m...\\nRead full review',\n",
       " 'Fantastic and prompt delivery.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph_full_review[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113711d2",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98df5c5",
   "metadata": {},
   "source": [
    "1.\tBrand \n",
    "2.\tProduct Description \n",
    "3.\tPrice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5a1545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart.com page and automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "17952628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching element\n",
    "\n",
    "product=driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "product.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "50e360b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search product by click\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME, \"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "46adecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap sneakers brand\n",
    "\n",
    "snk_brand=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fa04b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sneakers brand from the given page\n",
    "st=0\n",
    "ed=3\n",
    "\n",
    "\n",
    "for page in range(st, ed):\n",
    "    \n",
    "    sneakers = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in sneakers[0:-7]:\n",
    "        snk_brand.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_3 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_3.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8919f7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K- FOOTLANCE',\n",
       " 'BRUTON',\n",
       " 'BRUTON',\n",
       " 'Wildcraft',\n",
       " 'Sparx',\n",
       " 'PUMA',\n",
       " 'Elevarse',\n",
       " 'BIRDE',\n",
       " 'Shozie',\n",
       " 'K- FOOTLANCE',\n",
       " 'Shozie',\n",
       " 'RapidBox',\n",
       " 'Deals4you',\n",
       " 'aadi',\n",
       " 'aadi',\n",
       " 'Krors',\n",
       " 'World Wear Footwear',\n",
       " 'asian',\n",
       " 'WOODLAND',\n",
       " 'Layasa',\n",
       " 'HRX by Hrithik Roshan',\n",
       " 'World Wear Footwear',\n",
       " 'World Wear Footwear',\n",
       " 'Sparx',\n",
       " 'BRUTON',\n",
       " 'Chevit',\n",
       " 'BRUTON',\n",
       " 'Krors',\n",
       " 'RapidBox',\n",
       " 'Layasa',\n",
       " 'Shozie',\n",
       " 'bacca bucci',\n",
       " 'Layasa',\n",
       " 'asian',\n",
       " 'PUMA',\n",
       " 'RED TAPE',\n",
       " 'BRUTON',\n",
       " 'BRUTON',\n",
       " 'World Wear Footwear',\n",
       " 'RED TAPE',\n",
       " 'Labbin',\n",
       " 'World Wear Footwear',\n",
       " 'TR',\n",
       " 'Magnolia',\n",
       " 'luxury fashion',\n",
       " 'World Wear Footwear',\n",
       " 'Kraasa',\n",
       " 'BRUTON',\n",
       " 'K- FOOTLANCE',\n",
       " 'PUMA',\n",
       " 'Magnolia',\n",
       " 'Shozie',\n",
       " 'kardam&sons',\n",
       " 'Rzisbo',\n",
       " \"K' Footlance\",\n",
       " 'BRUTON',\n",
       " 'BRUTON',\n",
       " 'aadi',\n",
       " 'SCATCHITE',\n",
       " 'BRUTON',\n",
       " 'Extoes',\n",
       " 'aadi',\n",
       " 'Magnolia',\n",
       " 'tigonis',\n",
       " 'bacca bucci',\n",
       " 'World Wear Footwear',\n",
       " 'K- FOOTLANCE',\n",
       " 'BRUTON',\n",
       " 'BRUTON',\n",
       " 'Wildcraft',\n",
       " 'Sparx',\n",
       " 'PUMA',\n",
       " 'Elevarse',\n",
       " 'BIRDE',\n",
       " 'Shozie',\n",
       " 'K- FOOTLANCE',\n",
       " 'Shozie',\n",
       " 'RapidBox',\n",
       " 'Deals4you',\n",
       " 'aadi',\n",
       " 'aadi',\n",
       " 'Krors',\n",
       " 'World Wear Footwear',\n",
       " 'asian',\n",
       " 'WOODLAND',\n",
       " 'Layasa',\n",
       " 'HRX by Hrithik Roshan',\n",
       " 'World Wear Footwear',\n",
       " 'World Wear Footwear',\n",
       " 'Sparx',\n",
       " 'BRUTON',\n",
       " 'Chevit',\n",
       " 'BRUTON',\n",
       " 'Krors',\n",
       " 'RapidBox',\n",
       " 'Layasa',\n",
       " 'Shozie',\n",
       " 'Xylus',\n",
       " 'Layasa']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snk_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "75835762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap sneakers features\n",
    "\n",
    "snk_featu=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "464c5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sneakers features from the given page\n",
    "s=0\n",
    "e=3\n",
    "\n",
    "\n",
    "for page in range(s, e):\n",
    "    \n",
    "    sneakers_fea = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in sneakers_fea[0:-6]:\n",
    "        snk_featu.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_4 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_4.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7d71da76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sneakers For Men',\n",
       " 'Stylish & Trending Outdoor Walking Comfortable Sneakers...',\n",
       " '2 Combo Sneaker Shoes Sneakers For Men',\n",
       " 'True Black Shoe Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " '2 Combo Sneaker Shoes Sneakers For Men',\n",
       " 'Men White Sneakers Sneakers For Men',\n",
       " 'Premium Casual Shoes White Sneakers For Men',\n",
       " 'RS-FAST GO FOR Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Synthetic Leather Casual Sneaker shoes for Mens Sneaker...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Puma Smash Vulc Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Collection-1138 Stylish Casual Sports Sneakers F...',\n",
       " 'Skypy-31 Walking Shoes,Training Shoes,Sneakers,Loafers,...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Exclusive Affordable Collection of Trendy & Styl...',\n",
       " 'SM-322 Sneakers For Men',\n",
       " 'Combo Pack Of 2 Latest Stylish Casual Shoes for Men Lac...',\n",
       " 'Super Stylish & Trendy Combo Pack of 02 Pairs Sneakers ...',\n",
       " 'Sneakers For Men',\n",
       " \"Men's Casual Walking Partywear Sneakers Running White S...\",\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'STREET SAMURAI High top Flat high Street Fashion Sneake...',\n",
       " 'Sneakers For Men',\n",
       " \"Newton-01 Men's Lightweight Running Shoes Sneakers For ...\",\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Modern Trendy Sneakers Shoes Sneakers For Men',\n",
       " 'Lightweight Pack Of 1 Trendy Sneakers Sneakers For Men',\n",
       " 'Latest Collection-1215 Stylish Casual Sports Sneakers F...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Collection Black-349 Trendy & Stylish Casual Sne...',\n",
       " 'Sneakers For Men',\n",
       " 'luxuryfashion fashionable casual sneaker shoes white Sn...',\n",
       " 'Latest Collection-1216 Stylish Casual Sports Sneakers F...',\n",
       " 'Casuals, Canvas, Partywear Sneakers For Men',\n",
       " 'Lattest Sneakers Shoe Sneakers For Men',\n",
       " '2 Combo Sneaker Shoes Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Morden Comfertable Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Fashion and Stylish Soft Ultralight Lace Up Sneakers Ca...',\n",
       " 'Modern Trendy Sneakers Shoes Sneakers For Men',\n",
       " 'DEMON High-top casual shoe with Flat outsole high Stree...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers Sneakers For Men',\n",
       " 'Sneaker Sneakers For Men',\n",
       " 'Black Gym/Walking/Running Sports Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Tigonis Casuals For Men Sneakers For Men (White) Sneake...',\n",
       " 'Mesh | Ultralightweight | Comfortable | Breathable Walk...',\n",
       " \"Killer Men's Black Faux Leather Casual Lace Up Sneakers...\",\n",
       " 'Exclusive Affordable Collection of Trendy & Stylish Cas...',\n",
       " 'Electron E Pro Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Stylish & Trending Outdoor Walking Comfortable Sneakers...',\n",
       " '2 Combo Sneaker Shoes Sneakers For Men',\n",
       " 'True Black Shoe Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " '2 Combo Sneaker Shoes Sneakers For Men',\n",
       " 'Men White Sneakers Sneakers For Men',\n",
       " 'Premium Casual Shoes White Sneakers For Men',\n",
       " 'RS-FAST GO FOR Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Synthetic Leather Casual Sneaker shoes for Mens Sneaker...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Puma Smash Vulc Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Collection-1138 Stylish Casual Sports Sneakers F...',\n",
       " 'Skypy-31 Walking Shoes,Training Shoes,Sneakers,Loafers,...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Exclusive Affordable Collection of Trendy & Styl...',\n",
       " 'Casual Sneakers White Shoes For Girls And Sneakers For ...',\n",
       " 'Combo Pack Of 2 Latest Stylish Casual Shoes for Men Lac...',\n",
       " 'Super Stylish & Trendy Combo Pack of 02 Pairs Sneakers ...',\n",
       " 'Sneakers For Men',\n",
       " 'SM-322 Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " \"Men's Casual Walking Partywear Sneakers Running White S...\",\n",
       " 'Sneakers For Men']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snk_featu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f541095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap sneakers price\n",
    "snk_price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "057e5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sneakers features from the given page\n",
    "st_t=0\n",
    "en_d=3\n",
    "\n",
    "\n",
    "for page in range(st_t, en_d):\n",
    "    \n",
    "    sneakers_price = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in sneakers_price[0:-7]:\n",
    "        snk_price.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_5 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_5.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "742d5f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹239',\n",
       " '₹239',\n",
       " '₹399',\n",
       " '₹1,069',\n",
       " '₹583',\n",
       " '₹4,699',\n",
       " '₹233',\n",
       " '₹359',\n",
       " '₹287',\n",
       " '₹319',\n",
       " '₹260',\n",
       " '₹392',\n",
       " '₹344',\n",
       " '₹239',\n",
       " '₹216',\n",
       " '₹319',\n",
       " '₹143',\n",
       " '₹473',\n",
       " '₹1,298',\n",
       " '₹399',\n",
       " '₹1,275',\n",
       " '₹319',\n",
       " '₹143',\n",
       " '₹773',\n",
       " '₹393',\n",
       " '₹398',\n",
       " '₹399',\n",
       " '₹336',\n",
       " '₹449',\n",
       " '₹399',\n",
       " '₹288',\n",
       " '₹1,320',\n",
       " '₹319',\n",
       " '₹530',\n",
       " '₹2,249',\n",
       " '₹899',\n",
       " '₹191',\n",
       " '₹143',\n",
       " '₹143',\n",
       " '₹899',\n",
       " '₹319',\n",
       " '₹179',\n",
       " '₹291',\n",
       " '₹239',\n",
       " '₹1,159',\n",
       " '₹143',\n",
       " '₹342',\n",
       " '₹195',\n",
       " '₹299',\n",
       " '₹1,749',\n",
       " '₹239',\n",
       " '₹318',\n",
       " '₹303',\n",
       " '₹329',\n",
       " '₹239',\n",
       " '₹399',\n",
       " '₹399',\n",
       " '₹239',\n",
       " '₹288',\n",
       " '₹206',\n",
       " '₹231',\n",
       " '₹234',\n",
       " '₹239',\n",
       " '₹399',\n",
       " '₹1,414',\n",
       " '₹239',\n",
       " '₹239',\n",
       " '₹239',\n",
       " '₹399',\n",
       " '₹1,069',\n",
       " '₹583',\n",
       " '₹4,699',\n",
       " '₹233',\n",
       " '₹359',\n",
       " '₹287',\n",
       " '₹319',\n",
       " '₹260',\n",
       " '₹392',\n",
       " '₹344',\n",
       " '₹239',\n",
       " '₹216',\n",
       " '₹319',\n",
       " '₹143',\n",
       " '₹473',\n",
       " '₹1,298',\n",
       " '₹773',\n",
       " '₹1,275',\n",
       " '₹319',\n",
       " '₹143',\n",
       " '₹336',\n",
       " '₹393',\n",
       " '₹398',\n",
       " '₹399',\n",
       " '₹1,320',\n",
       " '₹449',\n",
       " '₹399',\n",
       " '₹288',\n",
       " '₹358',\n",
       " '₹319']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snk_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f6a74b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap sneakers discount percentage\n",
    "\n",
    "snk_disc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6e181d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sneakers features from the given page\n",
    "s_tt=0\n",
    "e_nd=3\n",
    "\n",
    "\n",
    "for page in range(s_tt, e_nd):\n",
    "    \n",
    "    sneakers_dis = driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in sneakers_dis[0:-7]:\n",
    "        snk_disc.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_6 = driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button_6.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "aa7a5ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59% off',\n",
       " '40% off',\n",
       " '82% off',\n",
       " '85% off',\n",
       " '76% off',\n",
       " '71% off',\n",
       " '82% off',\n",
       " '68% off',\n",
       " '64% off',\n",
       " '80% off',\n",
       " '76% off',\n",
       " '70% off',\n",
       " '71% off',\n",
       " '65% off',\n",
       " '84% off',\n",
       " '76% off',\n",
       " '65% off',\n",
       " '76% off',\n",
       " '68% off',\n",
       " '84% off',\n",
       " '67% off',\n",
       " '76% off',\n",
       " '69% off',\n",
       " '84% off',\n",
       " '88% off',\n",
       " '71% off',\n",
       " '84% off',\n",
       " '53% off',\n",
       " '76% off',\n",
       " '76% off',\n",
       " '73% off',\n",
       " '59% off',\n",
       " '52% off',\n",
       " '76% off',\n",
       " '81% off',\n",
       " '84% off',\n",
       " '58% off',\n",
       " '35% off',\n",
       " '53% off',\n",
       " '70% off',\n",
       " '76% off',\n",
       " '58% off',\n",
       " '68% off',\n",
       " '73% off',\n",
       " '60% off',\n",
       " '77% off',\n",
       " '88% off',\n",
       " '78% off',\n",
       " '68% off',\n",
       " '71% off',\n",
       " '40% off',\n",
       " '62% off',\n",
       " '32% off',\n",
       " '74% off',\n",
       " '84% off',\n",
       " '71% off',\n",
       " '66% off',\n",
       " '84% off',\n",
       " '79% off',\n",
       " '84% off',\n",
       " '62% off',\n",
       " '55% off',\n",
       " '60% off',\n",
       " '71% off',\n",
       " '64% off',\n",
       " '68% off',\n",
       " '59% off',\n",
       " '40% off',\n",
       " '82% off',\n",
       " '85% off',\n",
       " '76% off',\n",
       " '71% off',\n",
       " '82% off',\n",
       " '60% off',\n",
       " '64% off',\n",
       " '80% off',\n",
       " '76% off',\n",
       " '70% off',\n",
       " '71% off',\n",
       " '65% off',\n",
       " '84% off',\n",
       " '76% off',\n",
       " '65% off',\n",
       " '76% off',\n",
       " '68% off',\n",
       " '84% off',\n",
       " '67% off',\n",
       " '76% off',\n",
       " '69% off',\n",
       " '84% off',\n",
       " '88% off',\n",
       " '71% off',\n",
       " '84% off',\n",
       " '53% off',\n",
       " '76% off',\n",
       " '76% off',\n",
       " '73% off',\n",
       " '59% off',\n",
       " '52% off']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snk_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b0b804dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 97 99 99\n"
     ]
    }
   ],
   "source": [
    "print(len(snk_brand), len(snk_featu), len(snk_price), len(snk_disc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a2702",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes Set second Price filter and  Color filter to “Black”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7556d",
   "metadata": {},
   "source": [
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd491d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the Myntra.com page and automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.myntra.com/shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fb43d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "\n",
    "shoes_brand=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "daa15c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping shoes brand from the given page\n",
    "a=0\n",
    "b=4\n",
    "\n",
    "\n",
    "for page in range(a, b):\n",
    "    \n",
    "    shoes = driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    for i in shoes[0:-25]:\n",
    "        shoes_brand.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_7 = driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]')\n",
    "    next_button_7.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0b9be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shoes_brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19244377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shoes_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "274bb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "\n",
    "shoes_desc=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8ebca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping shoes description from the given page\n",
    "c=0\n",
    "d=4\n",
    "\n",
    "\n",
    "for page in range(c, d):\n",
    "    \n",
    "    shoes_ds = driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    for i in shoes_ds[0:-25]:\n",
    "        shoes_desc.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_8 = driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]')\n",
    "    next_button_8.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0eae1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shoes_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60a2425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shoes_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20ef743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "\n",
    "shoes_price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e8d5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping shoes price from the given page\n",
    "e=0\n",
    "f=4\n",
    "\n",
    "\n",
    "for page in range(e, f):\n",
    "    \n",
    "    shoes_pri = driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]')\n",
    "    for i in shoes_pri[0:-25]:\n",
    "        shoes_price.append(i.text)\n",
    "        \n",
    "    # scrap data from next page    \n",
    "    next_button_9 = driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]')\n",
    "    next_button_9.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db752309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shoes_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35c6fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shoes_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c043b2",
   "metadata": {},
   "source": [
    "MAKE DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e14de5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHOES_BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>SHOES_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>High-Top Platform Heeled Boots</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Solid Leather Boots</td>\n",
       "      <td>Rs. 6750Rs. 7500(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Women Block Heeled Boots</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men Leather Flat Boots</td>\n",
       "      <td>Rs. 5990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Block Heeled Boots</td>\n",
       "      <td>Rs. 8075Rs. 9500(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Clarks</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 5999Rs. 9999(40% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>TOMS</td>\n",
       "      <td>Women Leather Loafers</td>\n",
       "      <td>Rs. 8910Rs. 9900(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ruosh</td>\n",
       "      <td>Madrid Oiled Open Toe Flats</td>\n",
       "      <td>Rs. 6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DAVINCHI</td>\n",
       "      <td>Leather Calf Boots</td>\n",
       "      <td>Rs. 11990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ATESBER</td>\n",
       "      <td>Women Heeled Boots</td>\n",
       "      <td>Rs. 8010Rs. 8900(10% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SHOES_BRAND                     DESCRIPTION                SHOES_PRICE\n",
       "0     Skechers  High-Top Platform Heeled Boots                   Rs. 7999\n",
       "1     Skechers         Men Solid Leather Boots  Rs. 6750Rs. 7500(10% OFF)\n",
       "2         Puma        Women Block Heeled Boots                  Rs. 10999\n",
       "3       ADIDAS          Men Leather Flat Boots                   Rs. 5990\n",
       "4     Skechers              Block Heeled Boots  Rs. 8075Rs. 9500(15% OFF)\n",
       "..         ...                             ...                        ...\n",
       "95      Clarks       Men Leather Driving Shoes  Rs. 5999Rs. 9999(40% OFF)\n",
       "96        TOMS           Women Leather Loafers  Rs. 8910Rs. 9900(10% OFF)\n",
       "97       Ruosh     Madrid Oiled Open Toe Flats                   Rs. 6499\n",
       "98    DAVINCHI              Leather Calf Boots                  Rs. 11990\n",
       "99     ATESBER              Women Heeled Boots  Rs. 8010Rs. 8900(10% OFF)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "\n",
    "df_4 = pd.DataFrame({'SHOES_BRAND':shoes_brand, 'DESCRIPTION':shoes_desc, 'SHOES_PRICE':shoes_price})\n",
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b94ae8",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6bb69f",
   "metadata": {},
   "source": [
    "Then set CPU Type filter to “Intel Core i7” \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop: \n",
    "1. Title \n",
    "2.\tRatings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b251a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the amazon.in page and automated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6f5e0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "\n",
    "laptop_title=[]\n",
    "laptop_price=[]\n",
    "laptop_rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a79c0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping laptop title from the given page\n",
    "\n",
    "lap_tit = driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "for i in lap_tit[0:10]:\n",
    "    lt=i.text\n",
    "    laptop_title.append(lt)\n",
    "    \n",
    "    \n",
    "# scraping laptop price\n",
    "\n",
    "lap_pric = driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in lap_pric[0:10]:\n",
    "    lp=i.text\n",
    "    laptop_price.append(lp)\n",
    "    \n",
    "    \n",
    "# scraping company name\n",
    "\n",
    "lap_rati = driver.find_elements(By.XPATH, '//span[@class=\"a-size-base s-underline-text\"]')\n",
    "for i in lap_rati[0:10]:\n",
    "    lr=i.text\n",
    "    laptop_rating.append(lr)\n",
    "    \n",
    "time.sleep(5)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "708f78be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fujitsu UH-X 11th Gen Intel Core i7 13.3\" FHD IPS 400Nits 2-in1 Touch Convertible Laptop (16GB/1TB SSD/Windows 11/MS Office 2021/Iris Xe Graphics/2 Yr Warranty/Backlit Kb/Black/0.997 kg, 4ZR1F38027)',\n",
       " 'Lenovo Legion 5 Intel Core i7 11th Gen 15.6\" (39.62cm) FHD IPS Gaming Laptop (16GB/512GB SSD/4GB NVIDIA RTX 3050/120Hz/Windows 11/Office 2021/Backlit/3months Game Pass/Phantom Blue/2.4Kg), 82JK00GCIN',\n",
       " 'ASUS Zenbook 13 OLED, 13.3-inch (33.78 cms) FHD OLED, Intel EVO Core i7-1165G7 11th Gen, Thin and Light Laptop (16GB/1TB SSD/Iris Xe Graphics/Windows 11/Office 2021/Mist/1.14 kg), UX325EA-KG701WS',\n",
       " 'Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 15.6\" (39.62cm) FHD IPS Thin & Light Laptop (16GB/512GB SSD/Windows 11/Office 2021/Backlit/FPR/3months Game Pass/Storm Grey/1.85Kg), 82SF008WIN',\n",
       " 'Lenovo ThinkBook 15 Intel 11th Gen Core i7 15.6\"(39.62 cm)FHD Thin and Light Laptop (16GB/512GB SSD/Windows 11 Home/MS Office H&S 2021/Iris® Xe Graphics/Backlit/Mineral Grey/1.7 Kg) 20VE00W4IH',\n",
       " 'HP Pavilion Plus, 12th Gen Intel Core i7 16GB RAM/1TB SSD 14 inch(35.6 cm),OLED,400 nits,UWVA, Eye Safe Laptop/Intel Iris Xe Graphics/Backlit KB/B&O/FPR/Win 11/Alexa Built-in/MSO 2021, 14-eh0024TU',\n",
       " 'HP Pavilion x360 11th Gen Intel Core i7 14 inch(35.6 cm) FHD,IPS, Multitouch 2-in-1 Laptop(16GB RAM/512GB SSD/Backlit KB/Intel Iris Xe Graphics/Pen/Alexa Built-in/MSO/1.52Kg) 14-dy1050TU, Spruce Blue',\n",
       " 'Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 14 inches FHD IPS 2-in-1 Touchscreen Business Laptop (16GB/512GB SSD/Windows 10/MS Office/Digital Pen/Fingerprint Reader/Slate Grey/1.43Kg), 82BH004HIN',\n",
       " 'ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) FHD 144Hz, Intel Core i7-12700H 12th Gen, RTX 3050 Ti 4GB Graphics, Gaming Laptop (16GB/512GB SSD/Windows 11/Gray/2.2 kg), FX577ZE-HN056W',\n",
       " 'Hp Pavilion X360 11Th Gen Intel Core I7 14 Inches Fhd Multitouch 2In1 Laptop (16Gb Ram/512Gb Ssd/B&O/Windows 11 Home/Fpr/Backlit Kb/Intel Iris Xe Graphics/Pen/Alexa/Ms Office/Silver/1.52Kg)14-Dy1047Tu']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8cae1929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,00,000',\n",
       " '1,09,990',\n",
       " '93,290',\n",
       " '79,990',\n",
       " '77,990',\n",
       " '86,990',\n",
       " '80,990',\n",
       " '1,11,000',\n",
       " '99,990',\n",
       " '82,490']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "51002d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['84', '8', '12', '62', '39', '4', '89', '10', '26', '656']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a02ab",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e7f76",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps: \n",
    "1. First get the webpage https://www.ambitionbox.com/ \n",
    "2.\tClick on the salaries option as shown in the image. \n",
    "3.\tAfter reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. \n",
    "4.\tScrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required. \n",
    "5.\tStore the data in a dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c9c9af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the page and automated chrome browser\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0dd8f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list\n",
    "\n",
    "company_name=[]\n",
    "exp_req=[]\n",
    "min_max_salary=[]\n",
    "avg_salary=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1a960784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping  salary data of data scientist from the given page\n",
    "\n",
    "c_name = driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]')\n",
    "for i in c_name[0:10]:\n",
    "    cm=i.text\n",
    "    company_name.append(cm)\n",
    "    \n",
    "        \n",
    "    \n",
    "# scraping experiance\n",
    "\n",
    "ds_exp = driver.find_elements(By.XPATH, '//div[@class=\"sbold-list-header\"]')\n",
    "for i in ds_exp[0:10]:\n",
    "    dp=i.text\n",
    "    exp_req.append(dp)\n",
    "    \n",
    "    \n",
    "# scraping minimum and maximum salary\n",
    "\n",
    "min_max_sal = driver.find_elements(By.XPATH, '//div[@class=\"salary-values\"]')\n",
    "for i in min_max_sal[0:10]:\n",
    "    ms=i.text\n",
    "    min_max_salary.append(ms)\n",
    "    \n",
    "    \n",
    "# scraping average salary\n",
    "\n",
    "avg_sal = driver.find_elements(By.XPATH, '//p[@class=\"averageCtc\"]')\n",
    "for i in avg_sal[0:10]:\n",
    "    avs=i.text\n",
    "    avg_salary.append(avs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "time.sleep(5)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1454af3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walmart\\nData Scientist Salary\\n3-4 yrs experience (based on 24 salaries)',\n",
       " 'Ab Inbev\\nData Scientist Salary\\n2-4 yrs experience (based on 59 salaries)',\n",
       " 'Optum\\nData Scientist Salary\\n2-4 yrs experience (based on 49 salaries)',\n",
       " 'ZS\\nData Scientist Salary\\n1-2 yrs experience (based on 35 salaries)',\n",
       " 'Fractal Analytics\\nData Scientist Salary\\n2-4 yrs experience (based on 118 salaries)',\n",
       " 'Sigmoid Analytics\\nData Scientist Salary\\n1 yr experience (based on 10 salaries)',\n",
       " 'Tiger Analytics\\nData Scientist Salary\\n2-4 yrs experience (based on 70 salaries)',\n",
       " 'Legato Health Technologies\\nData Scientist Salary\\n4 yrs experience (based on 11 salaries)',\n",
       " 'HSBC\\nData Scientist Salary\\n4 yrs experience (based on 10 salaries)',\n",
       " 'Tredence\\nData Scientist Salary\\n3 yrs experience (based on 14 salaries)']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2a2c6f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-4 yrs experience (based on 24 salaries)',\n",
       " '2-4 yrs experience (based on 59 salaries)',\n",
       " '2-4 yrs experience (based on 49 salaries)',\n",
       " '1-2 yrs experience (based on 35 salaries)',\n",
       " '2-4 yrs experience (based on 118 salaries)',\n",
       " '1 yr experience (based on 10 salaries)',\n",
       " '2-4 yrs experience (based on 70 salaries)',\n",
       " '4 yrs experience (based on 11 salaries)',\n",
       " '4 yrs experience (based on 10 salaries)',\n",
       " '3 yrs experience (based on 14 salaries)']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "965457de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 25.0L\\n₹ 45.0L',\n",
       " '₹ 15.0L\\n₹ 26.0L',\n",
       " '₹ 11.0L\\n₹ 22.6L',\n",
       " '₹ 11.0L\\n₹ 22.0L',\n",
       " '₹ 9.0L\\n₹ 23.0L',\n",
       " '₹ 12.7L\\n₹ 19.7L',\n",
       " '₹ 9.0L\\n₹ 20.0L',\n",
       " '₹ 11.0L\\n₹ 20.0L',\n",
       " '₹ 12.0L\\n₹ 18.0L',\n",
       " '₹ 8.8L\\n₹ 17.5L']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "95e98ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 32.2L',\n",
       " '₹ 19.8L',\n",
       " '₹ 16.4L',\n",
       " '₹ 15.9L',\n",
       " '₹ 15.5L',\n",
       " '₹ 14.7L',\n",
       " '₹ 14.6L',\n",
       " '₹ 14.5L',\n",
       " '₹ 14.0L',\n",
       " '₹ 13.9L']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "80472aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# length of list\n",
    "print(len(company_name), len(exp_req), len(min_max_salary), len(avg_salary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d5dbb",
   "metadata": {},
   "source": [
    "MAKING DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a14780ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY_NAME</th>\n",
       "      <th>EXPERIANCE_REQUIRED</th>\n",
       "      <th>MIN_MAX_SALARY</th>\n",
       "      <th>AVERAGE_SALARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nData Scientist Salary\\n3-4 yrs experi...</td>\n",
       "      <td>3-4 yrs experience (based on 24 salaries)</td>\n",
       "      <td>₹ 25.0L\\n₹ 45.0L</td>\n",
       "      <td>₹ 32.2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nData Scientist Salary\\n2-4 yrs exper...</td>\n",
       "      <td>2-4 yrs experience (based on 59 salaries)</td>\n",
       "      <td>₹ 15.0L\\n₹ 26.0L</td>\n",
       "      <td>₹ 19.8L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum\\nData Scientist Salary\\n2-4 yrs experien...</td>\n",
       "      <td>2-4 yrs experience (based on 49 salaries)</td>\n",
       "      <td>₹ 11.0L\\n₹ 22.6L</td>\n",
       "      <td>₹ 16.4L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nData Scientist Salary\\n1-2 yrs experience ...</td>\n",
       "      <td>1-2 yrs experience (based on 35 salaries)</td>\n",
       "      <td>₹ 11.0L\\n₹ 22.0L</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics\\nData Scientist Salary\\n2-4 ...</td>\n",
       "      <td>2-4 yrs experience (based on 118 salaries)</td>\n",
       "      <td>₹ 9.0L\\n₹ 23.0L</td>\n",
       "      <td>₹ 15.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sigmoid Analytics\\nData Scientist Salary\\n1 yr...</td>\n",
       "      <td>1 yr experience (based on 10 salaries)</td>\n",
       "      <td>₹ 12.7L\\n₹ 19.7L</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics\\nData Scientist Salary\\n2-4 yr...</td>\n",
       "      <td>2-4 yrs experience (based on 70 salaries)</td>\n",
       "      <td>₹ 9.0L\\n₹ 20.0L</td>\n",
       "      <td>₹ 14.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Legato Health Technologies\\nData Scientist Sal...</td>\n",
       "      <td>4 yrs experience (based on 11 salaries)</td>\n",
       "      <td>₹ 11.0L\\n₹ 20.0L</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HSBC\\nData Scientist Salary\\n4 yrs experience ...</td>\n",
       "      <td>4 yrs experience (based on 10 salaries)</td>\n",
       "      <td>₹ 12.0L\\n₹ 18.0L</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tredence\\nData Scientist Salary\\n3 yrs experie...</td>\n",
       "      <td>3 yrs experience (based on 14 salaries)</td>\n",
       "      <td>₹ 8.8L\\n₹ 17.5L</td>\n",
       "      <td>₹ 13.9L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        COMPANY_NAME  \\\n",
       "0  Walmart\\nData Scientist Salary\\n3-4 yrs experi...   \n",
       "1  Ab Inbev\\nData Scientist Salary\\n2-4 yrs exper...   \n",
       "2  Optum\\nData Scientist Salary\\n2-4 yrs experien...   \n",
       "3  ZS\\nData Scientist Salary\\n1-2 yrs experience ...   \n",
       "4  Fractal Analytics\\nData Scientist Salary\\n2-4 ...   \n",
       "5  Sigmoid Analytics\\nData Scientist Salary\\n1 yr...   \n",
       "6  Tiger Analytics\\nData Scientist Salary\\n2-4 yr...   \n",
       "7  Legato Health Technologies\\nData Scientist Sal...   \n",
       "8  HSBC\\nData Scientist Salary\\n4 yrs experience ...   \n",
       "9  Tredence\\nData Scientist Salary\\n3 yrs experie...   \n",
       "\n",
       "                          EXPERIANCE_REQUIRED    MIN_MAX_SALARY AVERAGE_SALARY  \n",
       "0   3-4 yrs experience (based on 24 salaries)  ₹ 25.0L\\n₹ 45.0L        ₹ 32.2L  \n",
       "1   2-4 yrs experience (based on 59 salaries)  ₹ 15.0L\\n₹ 26.0L        ₹ 19.8L  \n",
       "2   2-4 yrs experience (based on 49 salaries)  ₹ 11.0L\\n₹ 22.6L        ₹ 16.4L  \n",
       "3   1-2 yrs experience (based on 35 salaries)  ₹ 11.0L\\n₹ 22.0L        ₹ 15.9L  \n",
       "4  2-4 yrs experience (based on 118 salaries)   ₹ 9.0L\\n₹ 23.0L        ₹ 15.5L  \n",
       "5      1 yr experience (based on 10 salaries)  ₹ 12.7L\\n₹ 19.7L        ₹ 14.7L  \n",
       "6   2-4 yrs experience (based on 70 salaries)   ₹ 9.0L\\n₹ 20.0L        ₹ 14.6L  \n",
       "7     4 yrs experience (based on 11 salaries)  ₹ 11.0L\\n₹ 20.0L        ₹ 14.5L  \n",
       "8     4 yrs experience (based on 10 salaries)  ₹ 12.0L\\n₹ 18.0L        ₹ 14.0L  \n",
       "9     3 yrs experience (based on 14 salaries)   ₹ 8.8L\\n₹ 17.5L        ₹ 13.9L  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe\n",
    "\n",
    "df_5 = pd.DataFrame({'COMPANY_NAME':company_name, 'EXPERIANCE_REQUIRED':exp_req, 'MIN_MAX_SALARY':min_max_salary, 'AVERAGE_SALARY':avg_salary})\n",
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555cd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
